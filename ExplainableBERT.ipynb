{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ccbbd839-32b0-4f52-b326-6dced062aceb",
      "metadata": {
        "id": "ccbbd839-32b0-4f52-b326-6dced062aceb"
      },
      "source": [
        "# Explainable Large Langague Model's\n",
        "\n",
        "**Author:** Lennox Anderson\n",
        "\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e8LLQ_dV702G1NieIQ1a1tuWPLdjiVpB?usp=sharing)\n",
        "\n",
        "**Null Hypothesis (H₀):**\n",
        "The fine-tuned LegalBERT model's predictions on legislative data are robust to minor perturbations in prompts, and saliency scores or counterfactual explanations do not reveal significant reliance on specific symbols, patterns, or individual words.\n",
        "\n",
        "**Alternative Hypothesis (H₁):**\n",
        "The fine-tuned LegalBERT model's predictions on legislative data are sensitive to minor prompt perturbations, saliency scores, or counterfactual explanations showing a strong dependence on certain symbols, patterns, or words, which may suggest bias or limited flexibility."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b074eb75-15d4-4076-888f-4cf7df30bfce",
      "metadata": {
        "id": "b074eb75-15d4-4076-888f-4cf7df30bfce"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Techniques for Explaining Fine-Tune Legal-BERT Model:**\n",
        "\n",
        "### Word Importance Analysis (Gradient-Based Saliency):\n",
        "<br>\n",
        "Measures which words matter the most when the model categorizes laws. Uses gradient calculations to score each words influence on the model decisions. For example, in civil rights laws, we check if \"rights\" or \"civil\" matters more.\n",
        "\n",
        "### Word Swap Testing (Perturbations):\n",
        "<br>\n",
        "Replaces key legal terms with similar words (like \"rights\" to \"privileges\") to test model stability. Helps us understand if the model relies too heavily on specific words or truly understands legal concepts.\n",
        "\n",
        "### Format Variation Testing (Counterfactuals):\n",
        "<br>\n",
        "Changes how laws are written while keeping core content (turning \"act\" into \"proposal\" or \"amendment\"). Tests if the model understands the meaning regardless of presentation format.\n",
        "\n",
        "### Statistical Validation:\n",
        "<br>\n",
        "Uses a scientifically valid sample of 382 laws from 49,747 total laws. Tests results against random chance using p-values to confirm findings are meaningful. All three methods combined show how well our model understands legal language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be049b0-5f24-41cf-9373-95af48363b7d",
      "metadata": {
        "id": "3be049b0-5f24-41cf-9373-95af48363b7d"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **0. Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05efcc64-e9c7-42ca-bd5b-815e54060825",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05efcc64-e9c7-42ca-bd5b-815e54060825",
        "outputId": "68fa55fa-8d6f-4ce0-8601-b9a2647e8946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8a7658-e018-40ef-8f02-323605448e16",
      "metadata": {
        "id": "5d8a7658-e018-40ef-8f02-323605448e16"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7998c5b-1b5a-410b-9ed0-dfbea4e5a81b",
      "metadata": {
        "id": "e7998c5b-1b5a-410b-9ed0-dfbea4e5a81b"
      },
      "source": [
        "---\n",
        "<br>\n",
        "\n",
        "### **1. Load the Fine-tune BERT(Legal)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5913a567-b40b-49f0-99d9-4906da5046e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5913a567-b40b-49f0-99d9-4906da5046e9",
        "outputId": "fe0c6e40-c028-48d2-f81f-ed37d1b0c43b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=117, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# load the label mapping from previous finetuning.\n",
        "label_file_path = './label_mapping.json'\n",
        "with open(label_file_path, 'r') as f:\n",
        "    label_mapping = json.load(f)\n",
        "    id_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# init the model and the tokenizer\n",
        "model_path = '/content/drive/MyDrive/fine_tuned_legalbert/'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbdb792-ab22-46b6-98bf-5d4e8fefb278",
      "metadata": {
        "id": "4fbdb792-ab22-46b6-98bf-5d4e8fefb278"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **2. Converting Legislative Text to Category Labels**\n",
        "\n",
        "This function is the foundation of our legislative text analysis system. It takes legislative text as input and outputs both a category prediction and a confidence score. Think of it as an automated legal document classifier - when you input text like \"An act concerning civil rights and liberties,\" it processes the text and determines the most likely legislative category, such as \"Law\" or \"Civil Rights.\" We use this function throughout our analysis as the base for testing the model's understanding and reliability. The included test texts represent different types of legislative documents, helping us evaluate how well the model handles various legal topics. Later in our code, this function helps us understand how changes to the text affect the model's decisions and measure its consistency across different variations of similar legislative language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8744d0-1106-4493-a0cd-6fa13e683db8",
      "metadata": {
        "id": "2e8744d0-1106-4493-a0cd-6fa13e683db8"
      },
      "outputs": [],
      "source": [
        "def get_model_prediction(text):\n",
        "   # convert text to tokens and move to gpu/mps\n",
        "   inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "   # disable gradient tracking to save memory during prediction\n",
        "   with torch.no_grad():\n",
        "       # pass tokens through model to get raw prediction scores\n",
        "       outputs = model(**inputs)\n",
        "       # convert raw scores to probabilities between 0-1\n",
        "       probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "       # get the predicted class number with highest probability\n",
        "       pred_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "   # return predicted class and its confidence score\n",
        "   return pred_class, probs[0][pred_class].item()\n",
        "\n",
        "# example texts covering different law types for testing\n",
        "texts = [\n",
        "    # civil rights example\n",
        "   \"An act concerning civil rights and liberties.\",\n",
        "    # commerce example\n",
        "   \"A bill to improve trade and commerce laws.\",\n",
        "    # security example\n",
        "   \"Legislation related to national security and defense.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7a7429-b544-449a-b672-ebced50dd8a7",
      "metadata": {
        "id": "fe7a7429-b544-449a-b672-ebced50dd8a7"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **3. Identifying Important Words in Legislative Text**\n",
        "This function reveals which words most influence our model's classification decisions. It analyzes legislative text and assigns importance scores to each word, showing us what the model focuses on when making its decisions. The process works by tracking how changes in each word would affect the model's final prediction - similar to understanding which parts of a legal document a human expert might highlight as crucial. We use this analysis to see if the model relies too heavily on specific words or if it considers the full context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a495962-6188-4537-9126-9a662e2eed2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a495962-6188-4537-9126-9a662e2eed2a",
        "outputId": "00437f97-0ccb-4f9c-aa8c-f8a279462a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saliency Analysis:\n",
            "Analyzing: 'An act concerning civil rights and liberties.'\n",
            "\n",
            "Predicted category: Law\n",
            "Confidence: 0.3548\n",
            "\n",
            "Token-wise importance scores:\n",
            "an              :   0.509141\n",
            "act             :   0.702962\n",
            "concerning      :   0.667069\n",
            "civil           :   0.000000\n",
            "rights          :   1.000000\n",
            "and             :   0.448494\n",
            "liberties       :   0.379502\n",
            ".               :   0.581452\n",
            "Saliency Analysis:\n",
            "Analyzing: 'A bill to improve trade and commerce laws.'\n",
            "\n",
            "Predicted category: Commerce\n",
            "Confidence: 0.5818\n",
            "\n",
            "Token-wise importance scores:\n",
            "a               :   0.616103\n",
            "bill            :   0.702334\n",
            "to              :   0.615817\n",
            "improve         :   1.000000\n",
            "trade           :   0.000000\n",
            "and             :   0.770755\n",
            "commerce        :   0.469304\n",
            "laws            :   0.728384\n",
            ".               :   0.715217\n",
            "Saliency Analysis:\n",
            "Analyzing: 'Legislation related to national security and defense.'\n",
            "\n",
            "Predicted category: Armed Forces and National Security\n",
            "Confidence: 0.9590\n",
            "\n",
            "Token-wise importance scores:\n",
            "legislation     :   0.540152\n",
            "related         :   0.306486\n",
            "to              :   0.320201\n",
            "national        :   0.505586\n",
            "security        :   1.000000\n",
            "and             :   0.406705\n",
            "defense         :   0.405122\n",
            ".               :   0.000000\n"
          ]
        }
      ],
      "source": [
        "def analyze_saliency(text):\n",
        "   print(\"Saliency Analysis:\")\n",
        "   print(f\"Analyzing: '{text}'\")\n",
        "\n",
        "   # tokenization process: converts text into tokens for the model\n",
        "   # adds special bert tokens, handles length limits, and adds padding\n",
        "   encoded = tokenizer.encode_plus(\n",
        "       text,\n",
        "       add_special_tokens=True,\n",
        "       max_length=512,\n",
        "       padding='max_length',\n",
        "       truncation=True,\n",
        "       return_tensors='pt'\n",
        "   )\n",
        "\n",
        "   # transfers encoded text to device for processing\n",
        "   input_ids = encoded['input_ids'].to(device)\n",
        "   attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "   # creates word embeddings and prepares for gradient tracking\n",
        "   # makes isolated copy of embeddings for importance calculation\n",
        "   embeddings = model.bert.embeddings.word_embeddings(input_ids)\n",
        "   input_embed = embeddings.clone().detach().to(device)\n",
        "   input_embed.requires_grad = True\n",
        "\n",
        "   # processes text through bert model to get classification logits\n",
        "   outputs = model.bert(inputs_embeds=input_embed, attention_mask=attention_mask)\n",
        "   logits = model.classifier(outputs[1])\n",
        "   pred = logits.argmax(dim=1).item()\n",
        "\n",
        "   # resets gradients and calculates new ones for prediction\n",
        "   # enables tracking of word importance in the decision\n",
        "   model.zero_grad()\n",
        "   logits[0, pred].backward()\n",
        "\n",
        "   # gets the importance scores through gradient * embedding multiplication\n",
        "   # indicates each word's contribution to the final prediction\n",
        "   gradients = input_embed.grad\n",
        "   attributions = (gradients * input_embed).sum(dim=2)\n",
        "\n",
        "   # normalizes importance scores to 0-1 range\n",
        "   # allows for standardized comparison between words\n",
        "   if attributions.max() != attributions.min():\n",
        "       attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
        "\n",
        "   # converts numeric tokens back to readable words\n",
        "   # removes padding and collects corresponding importance scores\n",
        "   tokens = tokenizer.convert_ids_to_tokens(input_ids.cpu().numpy()[0])\n",
        "   non_pad_mask = input_ids[0] != tokenizer.pad_token_id\n",
        "   valid_tokens = [t for t in tokens if t != tokenizer.pad_token]\n",
        "   valid_attributions = attributions.squeeze(0)[non_pad_mask].cpu().detach().numpy()\n",
        "\n",
        "   # outputs the predicted category and confidence level\n",
        "   print(f\"\\nPredicted category: {id_to_label[pred]}\")\n",
        "   probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "   print(f\"Confidence: {probs[0][pred]:.4f}\")\n",
        "\n",
        "   # displays importance score for each meaningful token\n",
        "   # excludes special tokens from the output\n",
        "   print(\"\\nToken-wise importance scores:\")\n",
        "   for token, score in zip(valid_tokens, valid_attributions):\n",
        "       if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "           print(f\"{token:15} : {score:10.6f}\")\n",
        "\n",
        "   # returns token-score pairs\n",
        "   return {'tokens': valid_tokens, 'scores': valid_attributions}\n",
        "\n",
        "# applies saliency analysis to each test text\n",
        "for text in texts:\n",
        "   analyze_saliency(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0937bc-f5a0-4554-b711-f9f2780c36cf",
      "metadata": {
        "id": "6e0937bc-f5a0-4554-b711-f9f2780c36cf"
      },
      "source": [
        "The saliency analysis above shows how the model weighs different words when categorizing legislative text. For civil rights legislation, the model focuses heavily on \"rights\" while surprisingly ignoring \"civil.\" In commerce-related text, \"improve\" and \"bill\" carry the most weight, though \"trade\" has less impact than expected. For security legislation, where the model shows its highest confidence at 95.90%, \"security\" is the key term. These scores, ranging from 0 to 1, reveal which words most influence the model's decisions - like a highlighter showing us what the model considers important. Interestingly, common words like \"and\" sometimes carry unexpected weight, while seemingly important topic words might score lower, giving us insights into how our model interprets legislative language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7cff0c-a302-4ee2-bb1a-dc8d62cf9d47",
      "metadata": {
        "id": "bf7cff0c-a302-4ee2-bb1a-dc8d62cf9d47"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **4. Testing Model Stability with Word Replacements**\n",
        "This function tests how well our model maintains its predictions when we swap key words with similar alternatives in legislative texts. It works by replacing specific legal terms with synonyms - for example, changing \"rights\" to \"privileges\" or \"commerce\" to \"business\" - and checking if these changes affect the model's category predictions. We use this to assess the model's stability and understanding of legal concepts beyond specific word choices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ecb21a-3b62-47c7-bcc1-6b122ff6d03f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ecb21a-3b62-47c7-bcc1-6b122ff6d03f",
        "outputId": "51ef35cf-f548-43c1-937b-3d97aa6ee37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perturbation Analysis:\n",
            "Original text: An act concerning civil rights and liberties.\n",
            "Original prediction: Law (confidence: 0.3548)\n",
            "\n",
            "Perturbation Results:\n",
            "\n",
            "Replaced 'rights' with 'privileges':\n",
            "New text: an act concerning civil privileges and liberties.\n",
            "New prediction: Law (confidence: 0.4547)\n",
            "Prediction changed: False\n",
            "\n",
            "Replaced 'civil' with 'public':\n",
            "New text: an act concerning public rights and liberties.\n",
            "New prediction: Government Operations and Politics (confidence: 0.2000)\n",
            "Prediction changed: True\n",
            "Perturbation Analysis:\n",
            "Original text: A bill to improve trade and commerce laws.\n",
            "Original prediction: Commerce (confidence: 0.5818)\n",
            "\n",
            "Perturbation Results:\n",
            "\n",
            "Replaced 'commerce' with 'business':\n",
            "New text: a bill to improve trade and business laws.\n",
            "New prediction: Commerce (confidence: 0.6470)\n",
            "Prediction changed: False\n",
            "\n",
            "Replaced 'trade' with 'exchange':\n",
            "New text: a bill to improve exchange and commerce laws.\n",
            "New prediction: Commerce (confidence: 0.6825)\n",
            "Prediction changed: False\n",
            "Perturbation Analysis:\n",
            "Original text: Legislation related to national security and defense.\n",
            "Original prediction: Armed Forces and National Security (confidence: 0.9590)\n",
            "\n",
            "Perturbation Results:\n",
            "\n",
            "Replaced 'security' with 'protection':\n",
            "New text: legislation related to national protection and defense.\n",
            "New prediction: Armed Forces and National Security (confidence: 0.9590)\n",
            "Prediction changed: False\n",
            "\n",
            "Replaced 'national' with 'federal':\n",
            "New text: legislation related to federal security and defense.\n",
            "New prediction: Armed Forces and National Security (confidence: 0.9534)\n",
            "Prediction changed: False\n"
          ]
        }
      ],
      "source": [
        "def analyze_perturbations(text):\n",
        "   # indicates start of analysis and displays original text\n",
        "   print(\"Perturbation Analysis:\")\n",
        "   print(f\"Original text: {text}\")\n",
        "\n",
        "   # gets baseline prediction before any word changes\n",
        "   orig_class, orig_conf = get_model_prediction(text)\n",
        "   print(f\"Original prediction: {id_to_label[orig_class]} (confidence: {orig_conf:.4f})\")\n",
        "\n",
        "   # defines pairs of words to swap in the analysis\n",
        "   # maps original legal terms to similar alternatives\n",
        "   perturbation_dict = {\n",
        "       \"rights\": \"privileges\",\n",
        "       \"commerce\": \"business\",\n",
        "       \"security\": \"protection\",\n",
        "       \"civil\": \"public\",\n",
        "       \"trade\": \"exchange\",\n",
        "       \"national\": \"federal\"\n",
        "   }\n",
        "\n",
        "   # tests each word replacement if present in text\n",
        "   results = []\n",
        "   for original, replacement in perturbation_dict.items():\n",
        "       if original.lower() in text.lower():\n",
        "           perturbed_text = text.lower().replace(original.lower(), replacement.lower())\n",
        "           pert_class, pert_conf = get_model_prediction(perturbed_text)\n",
        "           results.append({\n",
        "               'original_word': original,\n",
        "               'replacement': replacement,\n",
        "               'perturbed_text': perturbed_text,\n",
        "               'new_prediction': id_to_label[pert_class],\n",
        "               'confidence': pert_conf,\n",
        "               'changed': pert_class != orig_class\n",
        "           })\n",
        "\n",
        "   # shows if category changed and new confidence\n",
        "   print(\"\\nPerturbation Results:\")\n",
        "   for result in results:\n",
        "       print(f\"\\nReplaced '{result['original_word']}' with '{result['replacement']}':\")\n",
        "       print(f\"New text: {result['perturbed_text']}\")\n",
        "       print(f\"New prediction: {result['new_prediction']} (confidence: {result['confidence']:.4f})\")\n",
        "       print(f\"Prediction changed: {result['changed']}\")\n",
        "\n",
        "   return results\n",
        "\n",
        "# runs perturbation analysis on each example text\n",
        "for text in texts:\n",
        "   analyze_perturbations(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a16972-f2ec-4aa3-a118-bf6637d51ce6",
      "metadata": {
        "id": "b8a16972-f2ec-4aa3-a118-bf6637d51ce6"
      },
      "source": [
        "- The model shows interesting behaviors when we swap words in legislative texts. For the first text about civil rights, changing \"rights\" to \"privileges\" kept the \"Law\" classification but actually increased confidence to 45.47%. However, replacing \"civil\" with \"public\" completely changed the prediction to \"Government Operations.\" This tells us the word \"civil\" plays a crucial role in keeping the text in the \"Law\" category.\n",
        "\n",
        "- Looking at the commerce-related text, the model stayed steady. Whether we changed \"commerce\" to \"business\" or \"trade\" to \"exchange,\" it maintained its \"Commerce\" classification, even showing slightly higher confidence. This suggests the model understands these business terms are related.\n",
        "\n",
        "- The security legislation showed the strongest stability. Swapping \"security\" with \"protection\" or \"national\" with \"federal\" didn't change the prediction at all, and the confidence stayed very high at about 95%. This indicates the model has a robust understanding of security-related concepts beyond specific word choices.\n",
        "\n",
        "These results help us understand which words are crucial for classification and how flexible our model is with different word choices in legal language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dd2130-267a-4e04-9e93-796347bead6f",
      "metadata": {
        "id": "e0dd2130-267a-4e04-9e93-796347bead6f"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **5. Testing Model Consistency Across Different Legislative Formats using Counterfactual Analysis**\n",
        "This function examines how our model handles the same legislative content when it's presented in different formats. It takes a piece of legislation and rewrites it in various ways - turning an \"act\" into a \"proposal,\" \"amendment,\" or \"resolution\" while keeping the core content the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a2237c-9449-4d34-b855-40830a4ac7db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a2237c-9449-4d34-b855-40830a4ac7db",
        "outputId": "876afa86-bda9-4dc3-dd96-e58343a55e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Different Ways to Write Laws\n",
            "Starting with: An act concerning civil rights and liberties.\n",
            "Model thinks this is: Law (sure by: 0.3548)\n",
            "\n",
            "Results after changing formats:\n",
            "\n",
            "Tried this version: A proposal concerning civil rights and liberties.\n",
            "Model predicted: Law (confidence: 0.2652)\n",
            "Did prediction change? False\n",
            "\n",
            "Tried this version: An amendment related to civil rights and liberties.\n",
            "Model predicted: Law (confidence: 0.3257)\n",
            "Did prediction change? False\n",
            "\n",
            "Tried this version: A resolution regarding civil rights and liberties.\n",
            "Model predicted: Law (confidence: 0.3070)\n",
            "Did prediction change? False\n",
            "Testing Different Ways to Write Laws\n",
            "Starting with: A bill to improve trade and commerce laws.\n",
            "Model thinks this is: Commerce (sure by: 0.5818)\n",
            "\n",
            "Results after changing formats:\n",
            "\n",
            "Tried this version: A proposal concerning improve trade and commerce laws.\n",
            "Model predicted: Foreign Trade and International Finance (confidence: 0.3913)\n",
            "Did prediction change? True\n",
            "\n",
            "Tried this version: An amendment related to improve trade and commerce laws.\n",
            "Model predicted: Foreign Trade and International Finance (confidence: 0.4044)\n",
            "Did prediction change? True\n",
            "\n",
            "Tried this version: A resolution regarding improve trade and commerce laws.\n",
            "Model predicted: Foreign Trade and International Finance (confidence: 0.4073)\n",
            "Did prediction change? True\n",
            "Testing Different Ways to Write Laws\n",
            "Starting with: Legislation related to national security and defense.\n",
            "Model thinks this is: Armed Forces and National Security (sure by: 0.9590)\n",
            "\n",
            "Results after changing formats:\n",
            "\n",
            "Tried this version: A proposal concerning to national security and defense.\n",
            "Model predicted: Armed Forces and National Security (confidence: 0.9617)\n",
            "Did prediction change? False\n",
            "\n",
            "Tried this version: An amendment related to to national security and defense.\n",
            "Model predicted: Armed Forces and National Security (confidence: 0.9636)\n",
            "Did prediction change? False\n",
            "\n",
            "Tried this version: A resolution regarding to national security and defense.\n",
            "Model predicted: Armed Forces and National Security (confidence: 0.9604)\n",
            "Did prediction change? False\n"
          ]
        }
      ],
      "source": [
        "def test_different_formats(text, num_changes=3):\n",
        "   print(\"Testing Different Ways to Write Laws\")\n",
        "   print(f\"Starting with: {text}\")\n",
        "\n",
        "   # check what the model thinks about original text\n",
        "   original_type, original_confidence = get_model_prediction(text)\n",
        "   print(f\"Model thinks this is: {id_to_label[original_type]} (sure by: {original_confidence:.4f})\")\n",
        "\n",
        "   # list different ways to write the same law\n",
        "   different_formats = [\n",
        "       # try writing it as a proposal\n",
        "       lambda x: f\"A proposal concerning {x.lower().strip('.')}.\",\n",
        "       # try writing it as an amendment\n",
        "       lambda x: f\"An amendment related to {x.lower().strip('.')}.\",\n",
        "       # try writing it as a resolution\n",
        "       lambda x: f\"A resolution regarding {x.lower().strip('.')}.\",\n",
        "       # try writing it as a bill\n",
        "       lambda x: f\"A bill for {x.lower().strip('.')}.\",\n",
        "       # try writing it as legislation\n",
        "       lambda x: f\"Legislation to address {x.lower().strip('.')}.\",\n",
        "   ]\n",
        "\n",
        "   # remove the beginning part of the text that says what type of law it is\n",
        "   main_content = re.sub(r'^(an act|a bill|legislation|a resolution|an amendment)\\s+\\w+\\s+', '', text.lower())\n",
        "\n",
        "   # try each different format and save what happens\n",
        "   results = []\n",
        "   for i, new_format in enumerate(different_formats[:num_changes]):\n",
        "       # create new version with different format\n",
        "       new_version = new_format(main_content)\n",
        "       # see what model thinks about new version\n",
        "       new_type, new_confidence = get_model_prediction(new_version)\n",
        "       # save what happened\n",
        "       results.append({\n",
        "           'new_version': new_version,\n",
        "           'model_prediction': id_to_label[new_type],\n",
        "           'confidence': new_confidence,\n",
        "           'did_prediction_change': new_type != original_type\n",
        "       })\n",
        "\n",
        "   # show what happened with each change\n",
        "   print(\"\\nResults after changing formats:\")\n",
        "   for result in results:\n",
        "       print(f\"\\nTried this version: {result['new_version']}\")\n",
        "       print(f\"Model predicted: {result['model_prediction']} (confidence: {result['confidence']:.4f})\")\n",
        "       print(f\"Did prediction change? {result['did_prediction_change']}\")\n",
        "\n",
        "   # send back all results for later\n",
        "   return results\n",
        "\n",
        "# try this on all test\n",
        "for text in texts:\n",
        "   test_different_formats(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95a90e6-3b52-466e-a73a-a78bf2125da8",
      "metadata": {
        "id": "d95a90e6-3b52-466e-a73a-a78bf2125da8"
      },
      "source": [
        "The model reacts differently when we change how laws are written. For civil rights text, it keeps calling it \"Law\" even when we change \"act\" to \"proposal\" or \"amendment,\" though it becomes less sure (26-35% confidence). Commerce texts show weakness - changing \"bill\" to other formats makes the model switch from \"Commerce\" to \"Foreign Trade,\" suggesting format matters too much here. But security texts stay strong - the model keeps calling them \"Armed Forces and National Security\" with high confidence (96%) no matter how we write them. This shows the model understands security texts best, while it needs improvement on commerce topics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "038dd053-d57f-412a-8dc2-728a85ea1ef1",
      "metadata": {
        "id": "038dd053-d57f-412a-8dc2-728a85ea1ef1"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        " ### **6. Preparing Data for Legislative Statistical Analysis**\n",
        " This code sets up our legal text analysis tool and picks test examples. It loads our trained model that understands legal language, reads a file that matches numbers to law categories, and grabs our main dataset of bills. The code then picks 382 bills for testing, making sure to include examples from each category fairly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf053f9-2505-476c-8402-433d8af441b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bf053f9-2505-476c-8402-433d8af441b2",
        "outputId": "daaa9e5f-401e-4b6c-f6f0-d7f9dbd969ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "How many laws from each category:\n",
            "Category\n",
            "Government Operations and Politics                                                                          55\n",
            "Public Lands and Natural Resources                                                                          39\n",
            "Armed Forces and National Security                                                                          36\n",
            "Transportation and Public Works                                                                             36\n",
            "Law                                                                                                         19\n",
            "                                                                                                            ..\n",
            "Art and Memorials                                                                                            1\n",
            "Commemorative coins and minting typically fall under the category of Government Operations and Politics.     1\n",
            "Homeland Security                                                                                            1\n",
            "Insurance                                                                                                    1\n",
            "Telecommunications and Information                                                                           1\n",
            "Name: count, Length: 107, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b36b1ee90d5a>:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  fair_sample = data.groupby('Category', group_keys=False).apply(\n"
          ]
        }
      ],
      "source": [
        "# where to find our trained model and files\n",
        "# this is the model we trained earlier on legal texts\n",
        "model_folder = '/content/drive/MyDrive/fine_tuned_legalbert/'\n",
        "# this file tells us what number matches what type of law\n",
        "label_file = './label_mapping.json'\n",
        "\n",
        "# this has all the laws and their categories\n",
        "all_laws = pd.read_csv('./masterCategorized.csv')\n",
        "\n",
        "# creates a way to convert between numbers and law names\n",
        "with open(label_file, 'r') as file:\n",
        "   category_numbers = json.load(file)\n",
        "   # flip the dictionary so we can look up names from numbers\n",
        "   category_names = {v: k for k, v in category_numbers.items()}\n",
        "\n",
        "# get our model ready to use\n",
        "# tokenizer breaks text into pieces the model understands\n",
        "word_splitter = BertTokenizer.from_pretrained(model_folder)\n",
        "# this is our actual trained model that categorizes laws\n",
        "law_classifier = BertForSequenceClassification.from_pretrained(model_folder)\n",
        "# tell the model to use the apple gpu for faster processing\n",
        "gpu_device = torch.device(\"cuda\")\n",
        "law_classifier = law_classifier.to(gpu_device)\n",
        "# put model in testing mode\n",
        "law_classifier.eval()\n",
        "\n",
        "# this picks laws to test in a fair way\n",
        "def pick_test_laws(data, how_many=382):\n",
        "   \"\"\"picks laws from each category fairly\"\"\"\n",
        "   # group by category and pick some from each\n",
        "   fair_sample = data.groupby('Category', group_keys=False).apply(\n",
        "       lambda x: x.sample(n=max(1, int(how_many * len(x) / len(data))))\n",
        "   )\n",
        "   # make sure we don't get more than we asked for\n",
        "   return fair_sample.sample(n=min(len(fair_sample), how_many))\n",
        "\n",
        "# get our test laws\n",
        "test_laws = pick_test_laws(all_laws, how_many=382)\n",
        "\n",
        "# see how many laws we got from each category\n",
        "category_counts = test_laws['Category'].value_counts()\n",
        "print(\"\\nHow many laws from each category:\")\n",
        "print(category_counts)\n",
        "\n",
        "# we'll fill this with results from our three tests\n",
        "test_results = {\n",
        "    # for saliency test\n",
        "   'word_importance': defaultdict(list),\n",
        "    # for perturbation test\n",
        "   'word_swapping': defaultdict(list),\n",
        "    # for counterfactual test\n",
        "   'format_changes': defaultdict(list),\n",
        "    # everything together\n",
        "   'all_results': defaultdict(list)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93cfbcd-0847-4ef4-a55c-8eac91d3c353",
      "metadata": {
        "id": "b93cfbcd-0847-4ef4-a55c-8eac91d3c353"
      },
      "source": [
        "The test set of 382 bills shows how different types of laws are represented. Most bills (57) deal with Government Operations, followed by Armed Forces (37), Public Lands (36), and Transportation (32). Some categories like Sports and Military Logistics have just one bill each. This matches how real laws are distributed - some types are common, others rare. We use this mix to test how well our model handles both frequent and unusual types of legislation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d2a215-33a2-4fd8-9cb4-320dea452d2d",
      "metadata": {
        "id": "84d2a215-33a2-4fd8-9cb4-320dea452d2d"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **7. Analyzing 382 Bills to Represent 49,747 Laws**\n",
        "This function applies our three testing methods (saliency, perturbation, and counterfactual) to a carefully selected sample of 382 bills. We chose this sample size to reliably represent all 49,747 laws in our database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1ab40b-f4ae-46b7-a93b-8efbfcf21438",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec1ab40b-f4ae-46b7-a93b-8efbfcf21438",
        "outputId": "ffad1e1b-91ad-4df7-c7f1-0550cb22a608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing each law...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 382/382 [00:50<00:00,  7.49it/s]\n"
          ]
        }
      ],
      "source": [
        "def test_law_text(text, real_category):\n",
        "   # store all our results here\n",
        "   test_results = {}\n",
        "\n",
        "   # first break text into pieces the model can read\n",
        "   prepared_text = word_splitter.encode_plus(\n",
        "       text,\n",
        "       add_special_tokens=True,  # add special BERT markers\n",
        "       max_length=512,          # don't make it too long\n",
        "       padding='max_length',    # make all texts same length\n",
        "       truncation=True,         # cut if too long\n",
        "       return_tensors='pt'      # make it pytorch format\n",
        "   ).to(gpu_device)\n",
        "\n",
        "   # turn words into number patterns (vectors)\n",
        "   word_patterns = law_classifier.bert.embeddings.word_embeddings(prepared_text['input_ids'])\n",
        "   # make a copy we can track changes to\n",
        "   tracking_patterns = word_patterns.clone().detach().requires_grad_(True)\n",
        "\n",
        "   # run text through model to get prediction\n",
        "   model_output = law_classifier.bert(\n",
        "       inputs_embeds=tracking_patterns,\n",
        "       attention_mask=prepared_text['attention_mask']\n",
        "   )\n",
        "   raw_scores = law_classifier.classifier(model_output[1])\n",
        "   prediction = raw_scores.argmax(dim=1).item()\n",
        "\n",
        "   # prepare to check what affected prediction\n",
        "   law_classifier.zero_grad()\n",
        "   raw_scores[0, prediction].backward()\n",
        "\n",
        "   # calculate how much each word mattered\n",
        "   word_importance = (tracking_patterns.grad * tracking_patterns).sum(dim=2)\n",
        "   # make scores easier to understand (0 to 1)\n",
        "   if word_importance.max() != word_importance.min():\n",
        "       word_importance = (word_importance - word_importance.min()) / (word_importance.max() - word_importance.min())\n",
        "\n",
        "   # match scores back to words\n",
        "   words = word_splitter.convert_ids_to_tokens(prepared_text['input_ids'][0])\n",
        "   word_scores = {word: score.item() for word, score in zip(words, word_importance[0])\n",
        "                 if word not in ['[CLS]', '[SEP]', '[PAD]']}\n",
        "\n",
        "   swap_results = []\n",
        "   word_swaps = {\n",
        "       'act': 'bill',\n",
        "       'bill': 'act',\n",
        "       'rights': 'privileges',\n",
        "       'security': 'protection',\n",
        "       'commerce': 'business',\n",
        "       'national': 'federal'\n",
        "   }\n",
        "\n",
        "   # try each word swap that applies\n",
        "   for old_word, new_word in word_swaps.items():\n",
        "       if old_word.lower() in text.lower():\n",
        "           changed_text = text.lower().replace(old_word.lower(), new_word)\n",
        "           # see if swap changes prediction\n",
        "           with torch.no_grad():\n",
        "               new_input = word_splitter(changed_text, return_tensors='pt', padding=True, truncation=True).to(gpu_device)\n",
        "               new_output = law_classifier(**new_input)\n",
        "               new_prediction = new_output.logits.argmax(dim=1).item()\n",
        "               swap_results.append({\n",
        "                   'original_word': old_word,\n",
        "                   'replacement': new_word,\n",
        "                   'changed_prediction': new_prediction != prediction\n",
        "               })\n",
        "\n",
        "   format_changes = [\n",
        "       f\"A proposal concerning {text.lower()}\",\n",
        "       f\"An amendment related to {text.lower()}\",\n",
        "       f\"A resolution regarding {text.lower()}\"\n",
        "   ]\n",
        "\n",
        "   # check each new format\n",
        "   format_results = []\n",
        "   for new_format in format_changes:\n",
        "       with torch.no_grad():\n",
        "           format_input = word_splitter(new_format, return_tensors='pt', padding=True, truncation=True).to(gpu_device)\n",
        "           format_output = law_classifier(**format_input)\n",
        "           format_prediction = format_output.logits.argmax(dim=1).item()\n",
        "           format_results.append(format_prediction != prediction)\n",
        "\n",
        "   # package everything up\n",
        "   return {\n",
        "       'real_category': real_category,\n",
        "       'model_prediction': category_names[prediction],\n",
        "       'important_words': word_scores,\n",
        "       'word_swaps': swap_results,\n",
        "       'format_changes': format_results\n",
        "   }\n",
        "\n",
        "# test all our law texts\n",
        "all_test_results = []\n",
        "print(\"\\nTesting each law...\")\n",
        "for idx, row in tqdm(test_laws.iterrows(), total=len(test_laws)):\n",
        "   try:\n",
        "       # test this law and save results\n",
        "       result = test_law_text(row['Title'], row['Category'])\n",
        "       all_test_results.append(result)\n",
        "   except Exception as e:\n",
        "       print(f\"Problem with law {idx}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd966994-73c7-4050-a726-7a8d36014fe0",
      "metadata": {
        "id": "dd966994-73c7-4050-a726-7a8d36014fe0"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **8. Measuring Model Performance Across 382 Representative Bills**\n",
        "This function takes all our test results from the 382 bills and creates a clear picture of how well our model works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80bc26f2-09cb-41e3-99fc-95b2f2fdb1c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        },
        "id": "80bc26f2-09cb-41e3-99fc-95b2f2fdb1c6",
        "outputId": "9a595a44-d174-4ebb-943e-8c0b1ae208e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What We Learned\n",
            "Total Laws Tested: 382\n",
            "Model Got Right: 67.02%\n",
            "Changed Mind on Word Swaps: 14.29%\n",
            "Stayed Same with Format Changes: 94.50%\n",
            "\n",
            "Top 10 Words Model Cared About Most:\n",
            "coin: important 7 times\n",
            "appropriations: important 6 times\n",
            "national: important 6 times\n",
            "postal: important 6 times\n",
            "##age: important 5 times\n",
            "commission: important 5 times\n",
            "act: important 5 times\n",
            "authorize: important 4 times\n",
            "reservation: important 4 times\n",
            "indian: important 4 times\n",
            "Are Results Meaningful?\n",
            "Word Swap Test p-value: 0.0000\n",
            "Format Change Test p-value: 0.0000\n",
            "Final Thoughts:\n",
            "The model's behavior isn't random - it shows clear patterns in how it handles changes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def check_test_results(all_results):\n",
        "   # set up counters for everything we want to track\n",
        "   results_summary = {\n",
        "       'total_laws_tested': len(all_results),\n",
        "       'correct_predictions': 0,\n",
        "       'important_words_count': defaultdict(int),\n",
        "       'word_swap_changes': 0,\n",
        "       'format_stability': 0\n",
        "   }\n",
        "\n",
        "   # count how many times model got the category right\n",
        "   correct_guesses = sum(1 for result in all_results\n",
        "                        if result['real_category'] == result['model_prediction'])\n",
        "   results_summary['correct_predictions'] = correct_guesses / len(all_results)\n",
        "\n",
        "   # find which words were most important to the model\n",
        "   all_word_scores = []\n",
        "   for result in all_results:\n",
        "       # find the most important word in each law\n",
        "       most_important = max(result['important_words'].items(), key=lambda x: x[1])[0]\n",
        "       results_summary['important_words_count'][most_important] += 1\n",
        "       all_word_scores.extend(result['important_words'].values())\n",
        "\n",
        "   # see how often word swaps changed predictions\n",
        "   prediction_changes = []\n",
        "   for result in all_results:\n",
        "       # check if any word swap changed the prediction\n",
        "       changes = [p['changed_prediction'] for p in result['word_swaps']]\n",
        "       if changes:\n",
        "           prediction_changes.append(any(changes))\n",
        "   results_summary['word_swap_changes'] = sum(prediction_changes) / len(prediction_changes)\n",
        "\n",
        "   # check how stable predictions were with different formats\n",
        "   format_changes = []\n",
        "   for result in all_results:\n",
        "       format_changes.extend(result['format_changes'])\n",
        "   results_summary['format_stability'] = 1 - (sum(format_changes) / len(format_changes))\n",
        "\n",
        "   return results_summary, all_word_scores\n",
        "\n",
        "# get the summary of our tests\n",
        "results_summary, word_scores = check_test_results(all_test_results)\n",
        "\n",
        "# show what we found\n",
        "print(\"What We Learned\")\n",
        "print(f\"Total Laws Tested: {results_summary['total_laws_tested']}\")\n",
        "print(f\"Model Got Right: {results_summary['correct_predictions']:.2%}\")\n",
        "print(f\"Changed Mind on Word Swaps: {results_summary['word_swap_changes']:.2%}\")\n",
        "print(f\"Stayed Same with Format Changes: {results_summary['format_stability']:.2%}\")\n",
        "\n",
        "print(\"\\nTop 10 Words Model Cared About Most:\")\n",
        "for word, times_important in sorted(results_summary['important_words_count'].items(),\n",
        "                                 key=lambda x: x[1], reverse=True)[:10]:\n",
        "   print(f\"{word}: important {times_important} times\")\n",
        "\n",
        "# check if results are scientifically meaningful\n",
        "from scipy import stats\n",
        "\n",
        "# see if word swap results could be random chance\n",
        "word_swap_pvalue = stats.binomtest(\n",
        "   int(results_summary['word_swap_changes'] * results_summary['total_laws_tested']),\n",
        "   results_summary['total_laws_tested'],\n",
        "   p=0.5\n",
        ").pvalue\n",
        "\n",
        "# see if format change results could be random chance\n",
        "format_pvalue = stats.binomtest(\n",
        "   int(results_summary['format_stability'] * results_summary['total_laws_tested']),\n",
        "   results_summary['total_laws_tested'],\n",
        "   p=0.5\n",
        ").pvalue\n",
        "\n",
        "print(\"Are Results Meaningful?\")\n",
        "print(f\"Word Swap Test p-value: {word_swap_pvalue:.4f}\")\n",
        "print(f\"Format Change Test p-value: {format_pvalue:.4f}\")\n",
        "\n",
        "# make final decision about the model\n",
        "print(\"Final Thoughts:\")\n",
        "if word_swap_pvalue < 0.05 or format_pvalue < 0.05:\n",
        "   print(\"The model's behavior isn't random - it shows clear patterns in how it handles changes\")\n",
        "   if results_summary['word_swap_changes'] > 0.3:\n",
        "       print(\"The model changes its mind too easily when we swap words\")\n",
        "   if results_summary['format_stability'] < 0.7:\n",
        "       print(\"The model gets confused too easily by different formats\")\n",
        "else:\n",
        "   print(\"The model handles changes well and seems to understand the laws properly\")\n",
        "\n",
        "# make a picture showing word importance scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(word_scores, bins=50, alpha=0.7)\n",
        "plt.title('How Important Words Were to the Model')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('How Many Words')\n",
        "plt.show()\n",
        "\n",
        "# save everything in a spreadsheet for future analyis\n",
        "results_table = pd.DataFrame(all_test_results)\n",
        "results_table.to_csv('detailed_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a5533f-b3d2-47eb-99ef-576ed227c3a4",
      "metadata": {
        "id": "07a5533f-b3d2-47eb-99ef-576ed227c3a4"
      },
      "source": [
        "### Key Results\n",
        "Our model correctly categorized 65.45% of bills - good but not great. It's very stable when we change formats (94.68% stability) and mostly keeps its predictions when we swap words (only 12.33% sensitivity to changes). Common legal terms like \"act,\" \"national,\" and \"fiscal\" strongly influence its decisions.\n",
        "Issues and Limitations\n",
        "\n",
        "### Accuracy needs improvement - getting 1 in 3 bills wrong is too high for legal work\n",
        "The model might rely too much on common words like \"act\"\n",
        "Our sample of 382 bills might miss rare law types\n",
        "Not all word combinations were tested\n",
        "\n",
        "### Hypothesis Result\n",
        "We rejected our null hypothesis because the p-values (0.0000) show our results aren't random chance. The model's behavior with word changes and different formats is statistically significant. While it handles format changes well, it still shows some unexpected reactions to word swaps, suggesting it doesn't fully understand legal language like a human expert would."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}